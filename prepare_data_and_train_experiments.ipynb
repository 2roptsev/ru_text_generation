{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m spacy download ru_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flat-white/.local/lib/python3.6/site-packages/OpenSSL/crypto.py:8: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.\n",
      "  from cryptography import utils, x509\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm.auto as tqd\n",
    "\n",
    "from ipywidgets import Output\n",
    "from IPython.display import display\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from subword_nmt.learn_bpe import learn_bpe\n",
    "from subword_nmt.apply_bpe import BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.model import RNNModel, PretrainedSpacyEmbedding\n",
    "from source.generator import ArgmaxGenerator, DistributionGenerator, BeamSearchGenerator\n",
    "from source.utils import Lang, spacy_lemmatizer\n",
    "from source.train_utils import SequenceNLLLoss, NewsDataset, padding_collater, split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: train.py [-h] [--data-path DATA_PATH] [--embedding-dim EMBEDDING_DIM]\r\n",
      "                [--hidden-dim HIDDEN_DIM] [--device-name DEVICE_NAME]\r\n",
      "                [--n-epochs N_EPOCHS] [--batch-size BATCH_SIZE]\r\n",
      "                [--checkpoint-path CHECKPOINT_PATH]\r\n",
      "\r\n",
      "Train RNN model for generation.\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  --data-path DATA_PATH\r\n",
      "                        path to file train sentences\r\n",
      "  --embedding-dim EMBEDDING_DIM\r\n",
      "                        embedding size\r\n",
      "  --hidden-dim HIDDEN_DIM\r\n",
      "                        hidden state size\r\n",
      "  --device-name DEVICE_NAME\r\n",
      "                        device name. to use cpu for training change value to\r\n",
      "                        any\r\n",
      "  --n-epochs N_EPOCHS\r\n",
      "  --batch-size BATCH_SIZE\r\n",
      "  --checkpoint-path CHECKPOINT_PATH\r\n",
      "                        path to save trained model\r\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data_path = \"data/headers_full.txt\"\n",
    "\n",
    "with open(news_data_path, 'r') as f:\n",
    "    news_data_raw = f.read()\n",
    "\n",
    "news_sentences = news_data_raw.split('. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'рпцз призвала вынести ленина из мавзолея и начать декоммунизацию'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spacy lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"ru_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_WORDS = 20000\n",
    "MAX_LENGTH = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Had 58604 sentences\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "frequencies = defaultdict(int)\n",
    "\n",
    "for sentence in news_sentences:\n",
    "    doc = nlp(str(sentence))\n",
    "    for word in doc:\n",
    "        frequencies[word.lemma_] += 1\n",
    "\n",
    "print('Had', len(news_sentences), 'sentences')\n",
    "\n",
    "min_count = sorted(frequencies.values(), key=lambda v: -frequencies[v])[N_WORDS]\n",
    "print(min_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 38481 sentences\n"
     ]
    }
   ],
   "source": [
    "news_sentences_filtered_prep = []\n",
    "for sentence in news_sentences:\n",
    "    doc = nlp(str(sentence))\n",
    "    if len(doc) > MAX_LENGTH:\n",
    "        continue\n",
    "    if not all([frequencies[word.lemma_] > min_count for word in doc]):\n",
    "        continue\n",
    "    news_sentences_filtered_prep.append(sentence)\n",
    "\n",
    "print('Filtered', len(news_sentences_filtered_prep), 'sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/headers_filtered.txt\", 'w') as of:\n",
    "    of.writelines([line + '\\n' for line in news_sentences_filtered_prep])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next launches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38481\n",
      "найдены тела пропавших моряков с американского эсминца\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/headers_filtered.txt\", 'r') as inf:\n",
    "    news_sentences_filtered = inf.readlines()\n",
    "    \n",
    "print(len(news_sentences_filtered))\n",
    "print(news_sentences_filtered[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BPE_VOCAB_SIZE = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "def tokenize(x):\n",
    "    return ' '.join(tokenizer.tokenize(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:07<00:00, 501.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# split and tokenize the data\n",
    "with open('data/train.ru', 'w') as f_dst:\n",
    "    for line in news_sentences:\n",
    "        line = line.strip()\n",
    "        f_dst.write(tokenize(line) + '\\n')\n",
    "\n",
    "# build and apply bpe vocs\n",
    "learn_bpe(open('data/train.ru'), open('data/bpe_rules.ru', 'w'), num_symbols=BPE_VOCAB_SIZE)\n",
    "bpe = BPE(open('data/bpe_rules.ru'))\n",
    "\n",
    "with open('data/train.bpe.ru', 'w') as f_out:\n",
    "    for line in open('data/train.ru'):\n",
    "        f_out.write(bpe.process_line(line.strip()) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next launches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58604\n",
      "рп@@ ц@@ з призвала вы@@ не@@ сти лен@@ ина из м@@ ав@@ зо@@ ле@@ я и нач@@ ать де@@ коммуни@@ зацию\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('data/train.bpe.ru', 'r') as f_in:\n",
    "    news_sentences_bpe = f_in.readlines()\n",
    "    \n",
    "print(len(news_sentences_bpe))\n",
    "print(news_sentences_bpe[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 38381\n",
      "Most frequent [(19024, 'в'), (7703, 'на'), (4399, 'о'), (4262, 'с'), (3567, 'за'), (3481, '-'), (3192, 'и'), (2837, 'из'), (2333, 'по'), (2333, 'россии')]\n",
      "Less frequent [(1, 'газзаева'), (1, 'зеркало'), (1, 'разделили'), (1, 'свердловскую'), (1, 'осу'), (1, 'двойню'), (1, 'пограничных'), (1, 'корпусу'), (1, 'боксерском'), (1, 'травм')]\n"
     ]
    }
   ],
   "source": [
    "# spacy lemmas lang (word-level)\n",
    "lang = Lang(tokenizer=lambda x: nlp(x.strip()), lemmatizer=lambda word: word.text)\n",
    "lang.addDocument(news_sentences_filtered)\n",
    "lang.getStat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 79\n",
      "Most frequent [(402838, ' '), (285032, 'о'), (259369, 'а'), (255837, 'и'), (210645, 'е'), (177186, 'р'), (173345, 'с'), (172203, 'н'), (147323, 'в'), (146285, 'т')]\n",
      "Less frequent [(90, 'q'), (100, '?'), (124, ':'), (153, 'j'), (193, '.'), (196, '!'), (246, 'z'), (392, 'x'), (516, 'v'), (630, '9')]\n"
     ]
    }
   ],
   "source": [
    "# char lang (char-level)\n",
    "char_lang = Lang(tokenizer=lambda x: x, lemmatizer=lambda x: x)\n",
    "char_lang.addDocument(news_sentences)\n",
    "char_lang.getStat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 4118\n",
      "Most frequent [(28281, 'в'), (12327, 'на'), (8125, 'с'), (7772, '-'), (7667, 'о'), (7200, 'и'), (5832, 'за'), (4401, 'из'), (4067, 'у@@'), (3937, 'е')]\n",
      "Less frequent [(1, 'goo@@'), (1, 'ook'), (1, 'ссажи@@'), (1, 'goog@@'), (1, 'суэ@@'), (1, ',@@'), (1, 'товал@@'), (1, 'илот@@'), (1, 'паци@@'), (1, 'ъяв@@')]\n"
     ]
    }
   ],
   "source": [
    "# BPE lang (char-comb-level)\n",
    "bpe_lang = Lang(tokenizer=lambda x: x.split(), lemmatizer=lambda x: x)\n",
    "bpe_lang.addDocument(news_sentences_bpe)\n",
    "bpe_lang.getStat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty embeddings, short dataset\n",
    "from source.utils import PAD_TOKEN_INDEX\n",
    "\n",
    "n_epochs = 2\n",
    "batch_size = 32\n",
    "learning_rate = 1e-2\n",
    "\n",
    "emb_dim = 128\n",
    "NUM_EMBEDDINGS = len(lang.index2word)\n",
    "empty_emb_layer = nn.Embedding(num_embeddings=NUM_EMBEDDINGS,\n",
    "                               embedding_dim=emb_dim)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = RNNModel(empty_emb_layer,\n",
    "                 hidden_dim=128).to(device)\n",
    "\n",
    "gens = [ArgmaxGenerator(lang.index2word), \n",
    "        DistributionGenerator(lang.index2word, k_max=30),\n",
    "        BeamSearchGenerator(lang.index2word, max_length=10, beam_width=3)]\n",
    "\n",
    "loss = SequenceNLLLoss(ignore_index=PAD_TOKEN_INDEX)\n",
    "opt = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "short_word_dataset = NewsDataset(\n",
    "    sentences=news_sentences_filtered[:5000],\n",
    "    tokenizer=lang.tokenizer,\n",
    "    lemmatizer=lang.lemmatizer,\n",
    "    word2index=lang.word2index\n",
    ")\n",
    "short_word_dataset_train, short_word_dataset_val = split_dataset(short_word_dataset)\n",
    "\n",
    "train_dataloader = DataLoader(short_word_dataset_train, batch_size=batch_size, shuffle=True, \n",
    "                              collate_fn=padding_collater)\n",
    "val_dataloader = DataLoader(short_word_dataset_val, batch_size=batch_size, shuffle=False, \n",
    "                            collate_fn=padding_collater)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae79ba389f4410fbfd64b7fd94d0373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90ce7714a52f4686a9dfdd0e5a9eb080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea3c04675c1e4aa89a247d673ddcaa94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1436b259607e4081b1863c0eca25b962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c99322ba7d6472786ebea5dd571b4ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(model, device, gens, loss, opt, train_dataloader, val_dataloader, n_epochs, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty embeddings, full dataset\n",
    "\n",
    "n_epochs = 2\n",
    "batch_size = 32\n",
    "learning_rate = 1e-2\n",
    "\n",
    "emb_dim = 64\n",
    "NUM_EMBEDDINGS = len(lang.index2word)\n",
    "empty_emb_layer = nn.Embedding(num_embeddings=NUM_EMBEDDINGS,\n",
    "                               embedding_dim=emb_dim)\n",
    "model = RNNModel(empty_emb_layer,\n",
    "                 hidden_dim=64).to(device)\n",
    "\n",
    "gens = [ArgmaxGenerator(lang.index2word, max_length=30), \n",
    "        DistributionGenerator(lang.index2word, k_max=30, max_length=30)]\n",
    "\n",
    "loss = SequenceNLLLoss(ignore_index=PAD_TOKEN_INDEX)\n",
    "opt = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "word_dataset = NewsDataset(\n",
    "    sentences=news_sentences_filtered,\n",
    "    tokenizer=lang.tokenizer,\n",
    "    lemmatizer=lang.lemmatizer,\n",
    "    word2index=lang.word2index\n",
    ")\n",
    "word_dataset_train, word_dataset_val = split_dataset(word_dataset)\n",
    "\n",
    "train_dataloader = DataLoader(word_dataset_train, batch_size=batch_size, shuffle=True, \n",
    "                              collate_fn=padding_collater)\n",
    "val_dataloader = DataLoader(word_dataset_val, batch_size=batch_size, shuffle=False, \n",
    "                            collate_fn=padding_collater)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3bf184edff64a6ea21c5076b7c478b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a50e2d3df544ada4b0326e74c5d870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0cffd03644d4a28baa28e30414c4275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285b8bd7a45b45c9b251560a8e5ef450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f78d075363c742e08ba2d1565c205879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(model, device, gens, loss, opt, train_dataloader, val_dataloader, n_epochs, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy embeddings fixed, full dataset\n",
    "\n",
    "n_epochs = 2\n",
    "val_freq = 1\n",
    "batch_size = 32\n",
    "learning_rate = 1e-2\n",
    "\n",
    "\n",
    "spacy_emb_layer = PretrainedSpacyEmbedding(nlp, lang.index2word)\n",
    "model = RNNModel(spacy_emb_layer, \n",
    "                 hidden_dim=64).to(device)\n",
    "\n",
    "gens = [ArgmaxGenerator(lang.index2word), \n",
    "        DistributionGenerator(lang.index2word, k_max=30),\n",
    "        BeamSearchGenerator(lang.index2word, beam_width=2)]\n",
    "\n",
    "loss = SequenceNLLLoss(ignore_index=PAD_TOKEN_INDEX)\n",
    "opt = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "word_dataset = NewsDataset(\n",
    "    sentences=news_sentences_filtered,\n",
    "    tokenizer=lang.tokenizer,\n",
    "    lemmatizer=lambda word: word.text,\n",
    "    word2index=lang.word2index\n",
    ")\n",
    "word_dataset_train, word_dataset_val = split_dataset(word_dataset)\n",
    "\n",
    "train_dataloader = DataLoader(word_dataset_train, batch_size=batch_size, shuffle=True, \n",
    "                              collate_fn=padding_collater)\n",
    "val_dataloader = DataLoader(word_dataset_val, batch_size=batch_size, shuffle=False, \n",
    "                            collate_fn=padding_collater)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17cd6f5c5fc54517a0d1b358d81c84cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96fcbbd19a2f4c149daf73b1d106dbfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eeee16116a64342896103389b0abb2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a56ee2f627f14ddaa114038af402dca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5819d7f552954b0b9200430b99eae5e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(model, device, gens, loss, opt, train_dataloader, val_dataloader, n_epochs, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "путин рассказал от возможности с на\n",
      "суд львова рассказала в планах к сша в с - лет\n",
      "в - рассказали о отсутствии с в в россии и сирии\n",
      "в сети сообщили подробности смерти подготовке газа\n",
      "на германии рассказали о освобождении в россии\n",
      "в киеве сообщили в падении гибели двух москве\n",
      "в сети возмутились подорожание и - за - на -\n",
      "сми военные сообщили о планах с трампа\n",
      "в сети обратили сроки с новой в россией в россией\n",
      "в россии учредили в сша\n"
     ]
    }
   ],
   "source": [
    "gen = DistributionGenerator(index2word=lang.index2word, k_max=20, \n",
    "                            max_length=30)\n",
    "for i in range(10):\n",
    "    print(gen.generate(model, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char\n",
    "\n",
    "n_epochs = 10\n",
    "val_freq = 1\n",
    "batch_size = 32\n",
    "learning_rate = 1e-2\n",
    "\n",
    "emb_dim = 256\n",
    "NUM_EMBEDDINGS = len(char_lang.index2word)\n",
    "empty_emb_layer = nn.Embedding(num_embeddings=NUM_EMBEDDINGS,\n",
    "                               embedding_dim=emb_dim)\n",
    "model = RNNModel(spacy_emb_layer, \n",
    "                 hidden_dim=512).to(device)\n",
    "\n",
    "gens = [ArgmaxGenerator(char_lang.index2word), \n",
    "        DistributionGenerator(char_lang.index2word, k_max=30),\n",
    "        BeamSearchGenerator(char_lang.index2word, beam_width=2)]\n",
    "\n",
    "loss = SequenceNLLLoss(ignore_index=PAD_TOKEN_INDEX)\n",
    "opt = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "char_dataset = NewsDataset(\n",
    "    sentences=news_sentences_filtered,\n",
    "    tokenizer=char_lang.tokenizer,\n",
    "    lemmatizer=char_lang.lemmatizer,\n",
    "    word2index=char_lang.word2index\n",
    ")\n",
    "char_dataset_train, char_dataset_val = split_dataset(char_dataset)\n",
    "\n",
    "train_dataloader = DataLoader(char_dataset_train, batch_size=batch_size, shuffle=True, \n",
    "                              collate_fn=padding_collater)\n",
    "val_dataloader = DataLoader(char_dataset_val, batch_size=batch_size, shuffle=False, \n",
    "                            collate_fn=padding_collater)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bpe\n",
    "from source.utils import PAD_TOKEN_INDEX\n",
    "\n",
    "val_freq = 1\n",
    "batch_size = 32\n",
    "\n",
    "emb_dim = 256\n",
    "hid_dim = 1024\n",
    "NUM_EMBEDDINGS = len(bpe_lang.index2word)\n",
    "empty_emb_layer = nn.Embedding(num_embeddings=NUM_EMBEDDINGS,\n",
    "                               embedding_dim=emb_dim)\n",
    "device = torch.device('cuda')\n",
    "model = RNNModel(empty_emb_layer, \n",
    "                 hidden_dim=hid_dim).to(device)\n",
    "\n",
    "gens = [ArgmaxGenerator(bpe_lang.index2word, max_length=100), \n",
    "        DistributionGenerator(bpe_lang.index2word, max_length=100, k_max=20)]\n",
    "\n",
    "loss = SequenceNLLLoss(ignore_index=PAD_TOKEN_INDEX)\n",
    "\n",
    "bpe_dataset = NewsDataset(\n",
    "    sentences=news_sentences_bpe,\n",
    "    tokenizer=bpe_lang.tokenizer,\n",
    "    lemmatizer=bpe_lang.lemmatizer,\n",
    "    word2index=bpe_lang.word2index\n",
    ")\n",
    "bpe_dataset_train, bpe_dataset_val = split_dataset(bpe_dataset)\n",
    "\n",
    "train_dataloader = DataLoader(bpe_dataset_train, batch_size=batch_size, shuffle=True, \n",
    "                              collate_fn=padding_collater)\n",
    "val_dataloader = DataLoader(bpe_dataset_val, batch_size=batch_size, shuffle=False, \n",
    "                            collate_fn=padding_collater)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e1e174351df41cc94119090ca836538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70e279f3c5814b7d9c1675d17d9f7b0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4912b48e190d4d55ba2b387cbaa6fde6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "853bb82179dd46ceb84f8bbe2f1d3a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c768ac7a2448de99f2b789879c5a38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2037470c276465ba6d3130c717e27c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5acdad24f7f407888811b340a614ece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "train(model, gens, loss, opt, train_dataloader, val_dataloader, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d268b2546a44d1186a6d0928150035c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32aeb11b07fc4ae3963db6de34235aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d9c4a463dda47ac96a2fae7fb28617b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "opt = optim.Adam(model.parameters(), lr=1e-4)\n",
    "train(model, gens, loss, opt, train_dataloader, val_dataloader, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "в москве задержали 286 - го полицейских за несколько недель\n"
     ]
    }
   ],
   "source": [
    "gen = DistributionGenerator(bpe_lang.index2word, max_length=150, k_max=20)\n",
    "for i in range(1):\n",
    "    print(gen.generate(model, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (embedding): Embedding(4118, 256)\n",
       "  (lstm): LSTM(256, 1024, num_layers=2, batch_first=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (index_projection): Linear(in_features=1024, out_features=4118, bias=True)\n",
       "  (softmax): LogSoftmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'bpe_4000_1024'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "emb_dim = 256\n",
    "hid_dim = 1024\n",
    "NUM_EMBEDDINGS = len(bpe_lang.index2word)\n",
    "empty_emb_layer = nn.Embedding(num_embeddings=NUM_EMBEDDINGS,\n",
    "                               embedding_dim=emb_dim)\n",
    "model = RNNModel(empty_emb_layer, NUM_EMBEDDINGS, \n",
    "                 embedding_dim=emb_dim,\n",
    "                 hidden_dim=hid_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (embedding): Embedding(4118, 256)\n",
       "  (lstm): LSTM(256, 1024, num_layers=2, batch_first=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (index_projection): Linear(in_features=1024, out_features=4118, bias=True)\n",
       "  (softmax): LogSoftmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = DistributionGenerator(bpe_lang.index2word, max_length=150, k_max=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'в совфеде оценили шансы на идею путина и трампа про борьбу с россией'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.generate(model, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
